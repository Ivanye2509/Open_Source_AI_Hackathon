{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Architecture Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "graph TD\n",
    "    \n",
    "    A[Social Media Data] --> B[Data Preprocessing]\n",
    "    B --> C[Vector Database]\n",
    "    C --> D{User Query}\n",
    "    D --> E[Relevant Context Retrieval]\n",
    "    E --> F[LLM + Persona Prompt]\n",
    "    F --> G[Persona-Chat Response]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Implementation Steps\n",
    "### a. Data Collection & Preprocessing\n",
    "### 1. Data Sources (simulated or real):\n",
    "\n",
    "Facebook/Instagram posts\n",
    "\n",
    "Twitter/X threads\n",
    "\n",
    "Reddit comments\n",
    "\n",
    "LinkedIn articles\n",
    "\n",
    "### 2. Preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "def preprocess_data(raw_data):\n",
    "    # Clean text (emojis, URLs, special chars)\n",
    "    clean_text = re.sub(r'http\\S+', '', raw_data)\n",
    "    clean_text = clean_text.encode('ascii', 'ignore').decode()\n",
    "    \n",
    "    # Split into chunks with metadata\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=200,\n",
    "        length_function=len\n",
    "    )\n",
    "    docs = text_splitter.create_documents([clean_text])\n",
    "    \n",
    "    # Add metadata (platform, date, likes)\n",
    "    for doc in docs:\n",
    "        doc.metadata.update({\n",
    "            \"source\": \"twitter\",\n",
    "            \"date\": \"2023-03-15\",\n",
    "            \"engagement\": 42\n",
    "        })\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. Vector Database Setup\n",
    "1. Embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embedder = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Vector Store (ChromaDB example):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "\n",
    "client = chromadb.PersistentClient(path=\"./persona_db\")\n",
    "collection = client.create_collection(\"social_media\")\n",
    "\n",
    "# Store processed data\n",
    "collection.add(\n",
    "    documents=[doc.page_content for doc in processed_docs],\n",
    "    metadatas=[doc.metadata for doc in processed_docs],\n",
    "    ids=[f\"doc_{i}\" for i in range(len(processed_docs))]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. Persona Retrieval System\n",
    "1. Hybrid Search (semantic + engagement filtering):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_persona_context(query, top_k=5):\n",
    "    # Semantic search\n",
    "    results = collection.query(\n",
    "        query_texts=[query],\n",
    "        n_results=top_k,\n",
    "        where={\"source\": {\"$in\": [\"twitter\", \"facebook\"]}},  # Filter by platform\n",
    "        where_document={\"$contains\": query.split()[0]}  # Partial match\n",
    "    )\n",
    "    \n",
    "    # Boost popular posts\n",
    "    sorted_results = sorted(\n",
    "        zip(results['documents'][0], results['metadatas'][0]),\n",
    "        key=lambda x: x[1]['engagement'],\n",
    "        reverse=True\n",
    "    )\n",
    "    return [text for text, _ in sorted_results[:3]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d. Persona Prompt Engineering\n",
    "1. Dynamic Prompt Template:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "persona_template = \"\"\"\n",
    "You are {name}'s digital twin. Respond as they would based on their historical social media style:\n",
    "\n",
    "**Key Personality Traits** (extracted from data):\n",
    "- Speech style: {speech_style}\n",
    "- Frequently used phrases: {common_phrases}\n",
    "- Topics of interest: {top_topics}\n",
    "\n",
    "**Current Context**:\n",
    "{retrieved_context}\n",
    "\n",
    "**Current Conversation**:\n",
    "User: {input}\n",
    "AI Persona: \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Personality Extraction (automated):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_personality(docs):\n",
    "    analyzer_prompt = \"\"\"Analyze this text and extract:\n",
    "    1. 3 speech style adjectives (e.g., 'sarcastic')\n",
    "    2. 5 common phrases\n",
    "    3. Top 3 topics\"\"\"\n",
    "    \n",
    "    analysis = llm.invoke(analyzer_prompt + \"\\n\".join(docs))\n",
    "    return parse_analysis(analysis)  # Implement parsing logic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e. Chat Interface\n",
    "1. Full RAG Pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "def persona_chat(query, user_data):\n",
    "    # Retrieve context\n",
    "    context = retrieve_persona_context(query)\n",
    "    \n",
    "    # Analyze personality\n",
    "    traits = analyze_personality(user_data)\n",
    "    \n",
    "    # Build prompt\n",
    "    prompt = ChatPromptTemplate.from_template(persona_template).format(\n",
    "        name=\"John Doe\",\n",
    "        speech_style=traits['style'],\n",
    "        common_phrases=\", \".join(traits['phrases']),\n",
    "        top_topics=\", \".join(traits['topics']),\n",
    "        retrieved_context=\"\\n- \".join(context),\n",
    "        input=query\n",
    "    )\n",
    "    \n",
    "    # Generate response\n",
    "    llm = ChatOpenAI(model=\"gpt-4\", temperature=0.7)\n",
    "    return LLMChain(llm=llm, prompt=prompt).run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Example Workflow\n",
    "User Query: \"What's your take on AI regulation?\"\n",
    "\n",
    "Retrieved Context:\n",
    "\n",
    "\"Tech companies shouldn't self-regulate AI - we need independent oversight (Twitter, 2023)\"\n",
    "\n",
    "\"Loving the new EU AI Act framework! üéâ (LinkedIn, 2024)\"\n",
    "\n",
    "\"AI ethics is complicated but crucial (Reddit comment, 2022)\"\n",
    "\n",
    "Generated Response:\n",
    "\"Honestly, I'm all for the EU's approach - independent oversight beats corporate self-regulation any day. Remember when I tweeted about this last year? It's complicated, but crucial we get it right. üß† #AIethics\"\n",
    "\n",
    "4. Advanced Features\n",
    "Memory Management:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"history\",\n",
    "    input_key=\"input\",\n",
    "    chat_memory=message_list  # Store past interactions\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Style Transfer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_transfer_prompt = \"\"\"\n",
    "Rephrase this in {name}'s style:\n",
    "Original: {response}\n",
    "Use their common phrases: {phrases}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Privacy Protection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from presidio_analyzer import AnalyzerEngine\n",
    "from presidio_anonymizer import AnonymizerEngine\n",
    "\n",
    "def anonymize_input(text):\n",
    "    analyzer = AnalyzerEngine()\n",
    "    anonymizer = AnonymizerEngine()\n",
    "    results = analyzer.analyze(text=text, language='en')\n",
    "    return anonymizer.anonymize(text, results).text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Evaluation Metrics\n",
    "\n",
    "Persona Consistency Score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_persona(response, original_data):\n",
    "    similarity_prompt = f\"\"\"\n",
    "    Rate 1-10 how similar this response is to the writing style below:\n",
    "    Response: {response}\n",
    "    Style Samples: {original_data[:1000]}\n",
    "    \"\"\"\n",
    "    return llm.invoke(similarity_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Engagement Metrics:\n",
    "\n",
    "Average response length\n",
    "\n",
    "Emoji/ slang usage match\n",
    "\n",
    "Topic alignment\n",
    "\n",
    "6. Tools & Deployment\n",
    "Quick Start with sample data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with simulated data\n",
    "mock_data = [\n",
    "    \"Just tried the new GPT-5 API - mind blown! ü§Ø #AI\",\n",
    "    \"Privacy laws need to catch up with AI development. Thoughts?\",\n",
    "    \"Sunday vibes: Coffee ‚òï, coding üíª, cat memes üêà\"\n",
    "]\n",
    "\n",
    "chat_response = persona_chat(\"What's your weekend plan?\", mock_data)\n",
    "print(f\"Persona Response: {chat_response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deployment Options:\n",
    "\n",
    "Gradio/Streamlit for UI\n",
    "\n",
    "FastAPI backend with auth\n",
    "\n",
    "AWS Lambda for serverless scaling\n",
    "\n",
    "7. Ethical Considerations\n",
    "User Consent: Explicit opt-in for data usage\n",
    "\n",
    "Data Encryption: AES-256 for stored data\n",
    "\n",
    "Forgetting Mechanism:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_user_data(user_id):\n",
    "    collection.delete(where={\"user_id\": user_id})\n",
    "    os.remove(f\"./data/{user_id}.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This system creates a digital twin that mirrors a user's communication style while maintaining ethical standards. Adjust the retrieval strategy and personality extraction based on your specific use case!\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
